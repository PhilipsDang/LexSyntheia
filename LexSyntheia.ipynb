{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38e736-bfba-46f7-9fc6-e9580a59e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Core dependencies\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.retrievers import VectorStoreRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "import docx  # python-docx for Word documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2d809-4489-4ccb-b915-b76ec76b50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57369ddd-bf19-45cc-9f96-456511c20d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LegalQuery:\n",
    "    \"\"\"Structure for legal queries\"\"\"\n",
    "    query_id: str\n",
    "    user_id: str\n",
    "    query_text: str\n",
    "    query_type: str  # contract_analysis, legal_research, compliance_check, etc.\n",
    "    jurisdiction: str\n",
    "    urgency: str  # low, medium, high, urgent\n",
    "    timestamp: datetime\n",
    "    context: Dict[str, Any] = None\n",
    "\n",
    "@dataclass\n",
    "class LegalResponse:\n",
    "    \"\"\"Structure for legal responses\"\"\"\n",
    "    query_id: str\n",
    "    response_text: str\n",
    "    confidence_score: float\n",
    "    sources: List[Dict[str, str]]\n",
    "    legal_citations: List[str]\n",
    "    recommendations: List[str]\n",
    "    timestamp: datetime\n",
    "    processing_time: float\n",
    "\n",
    "class DeepSeekClient:\n",
    "    \"\"\"Client for DeepSeek R1 model integration\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, base_url: str = \"https://api.deepseek.com\"):\n",
    "        self.client = openai.OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        self.model = \"deepseek-r1\"\n",
    "    \n",
    "    async def generate_response(self, prompt: str, system_prompt: str = None, **kwargs) -> str:\n",
    "        \"\"\"Generate response using DeepSeek R1 model\"\"\"\n",
    "        try:\n",
    "            messages = []\n",
    "            if system_prompt:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=kwargs.get('temperature', 0.1),\n",
    "                max_tokens=kwargs.get('max_tokens', 2000),\n",
    "                top_p=kwargs.get('top_p', 0.9)\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response with DeepSeek: {e}\")\n",
    "            raise\n",
    "\n",
    "class LegalDocumentProcessor:\n",
    "    \"\"\"Process and parse legal documents\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF documents\"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text = \"\"\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "            doc.close()\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting text from PDF: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_text_from_docx(self, docx_path: str) -> str:\n",
    "        \"\"\"Extract text from Word documents\"\"\"\n",
    "        try:\n",
    "            doc = docx.Document(docx_path)\n",
    "            text = \"\"\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting text from DOCX: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def process_document(self, file_path: str, metadata: Dict[str, Any] = None) -> List[Document]:\n",
    "        \"\"\"Process document and return chunks\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        if file_path.suffix.lower() == '.pdf':\n",
    "            text = self.extract_text_from_pdf(str(file_path))\n",
    "        elif file_path.suffix.lower() in ['.docx', '.doc']:\n",
    "            text = self.extract_text_from_docx(str(file_path))\n",
    "        elif file_path.suffix.lower() == '.txt':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_path.suffix}\")\n",
    "        \n",
    "        # Split text into chunks\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        \n",
    "        # Create documents with metadata\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc_metadata = {\n",
    "                \"source\": str(file_path),\n",
    "                \"chunk_id\": i,\n",
    "                \"file_type\": file_path.suffix.lower(),\n",
    "                **(metadata or {})\n",
    "            }\n",
    "            documents.append(Document(page_content=chunk, metadata=doc_metadata))\n",
    "        \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ac120-6576-4818-a96e-c8d1f94867f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Retrieval-Augmented Generation system for legal knowledge\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_model: str = \"text-embedding-ada-002\"):\n",
    "        self.embeddings = OpenAIEmbeddings(model=embeddings_model)\n",
    "        self.vector_store = None\n",
    "        self.retriever = None\n",
    "        self.doc_processor = LegalDocumentProcessor()\n",
    "    \n",
    "    def build_knowledge_base(self, documents_path: str):\n",
    "        \"\"\"Build knowledge base from legal documents\"\"\"\n",
    "        logger.info(\"Building legal knowledge base...\")\n",
    "        \n",
    "        documents = []\n",
    "        documents_path = Path(documents_path)\n",
    "        \n",
    "        # Process all legal documents\n",
    "        for file_path in documents_path.rglob(\"*\"):\n",
    "            if file_path.is_file() and file_path.suffix.lower() in ['.pdf', '.docx', '.doc', '.txt']:\n",
    "                try:\n",
    "                    # Add metadata about document type\n",
    "                    metadata = {\n",
    "                        \"document_type\": self._classify_document_type(file_path.name),\n",
    "                        \"jurisdiction\": self._extract_jurisdiction(file_path.name),\n",
    "                        \"last_updated\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    docs = self.doc_processor.process_document(str(file_path), metadata)\n",
    "                    documents.extend(docs)\n",
    "                    logger.info(f\"Processed {len(docs)} chunks from {file_path.name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        \n",
    "        # Create vector store\n",
    "        if documents:\n",
    "            self.vector_store = FAISS.from_documents(documents, self.embeddings)\n",
    "            self.retriever = VectorStoreRetriever(\n",
    "                vectorstore=self.vector_store,\n",
    "                search_kwargs={\"k\": 5}\n",
    "            )\n",
    "            logger.info(f\"Knowledge base built with {len(documents)} document chunks\")\n",
    "        else:\n",
    "            logger.warning(\"No documents were processed for the knowledge base\")\n",
    "    \n",
    "    def _classify_document_type(self, filename: str) -> str:\n",
    "        \"\"\"Classify document type based on filename\"\"\"\n",
    "        filename_lower = filename.lower()\n",
    "        if any(word in filename_lower for word in ['contract', 'agreement', 'terms']):\n",
    "            return 'contract'\n",
    "        elif any(word in filename_lower for word in ['statute', 'law', 'code', 'act']):\n",
    "            return 'statute'\n",
    "        elif any(word in filename_lower for word in ['case', 'court', 'judgment', 'ruling']):\n",
    "            return 'case_law'\n",
    "        elif any(word in filename_lower for word in ['regulation', 'rule', 'policy']):\n",
    "            return 'regulation'\n",
    "        else:\n",
    "            return 'general'\n",
    "    \n",
    "    def _extract_jurisdiction(self, filename: str) -> str:\n",
    "        \"\"\"Extract jurisdiction from filename\"\"\"\n",
    "        filename_lower = filename.lower()\n",
    "        if any(word in filename_lower for word in ['federal', 'us', 'usa', 'united_states']):\n",
    "            return 'federal'\n",
    "        elif any(word in filename_lower for word in ['state', 'california', 'texas', 'new_york']):\n",
    "            return 'state'\n",
    "        elif any(word in filename_lower for word in ['china', 'chinese', '中国']):\n",
    "            return 'china'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    \n",
    "    def retrieve_relevant_documents(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
    "        if not self.retriever:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            docs = self.retriever.get_relevant_documents(query)\n",
    "            return docs[:k]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving documents: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def save_knowledge_base(self, path: str):\n",
    "        \"\"\"Save the vector store to disk\"\"\"\n",
    "        if self.vector_store:\n",
    "            self.vector_store.save_local(path)\n",
    "    \n",
    "    def load_knowledge_base(self, path: str):\n",
    "        \"\"\"Load the vector store from disk\"\"\"\n",
    "        try:\n",
    "            self.vector_store = FAISS.load_local(path, self.embeddings)\n",
    "            self.retriever = VectorStoreRetriever(\n",
    "                vectorstore=self.vector_store,\n",
    "                search_kwargs={\"k\": 5}\n",
    "            )\n",
    "            logger.info(\"Knowledge base loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading knowledge base: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23ddfc-d484-4cf9-a690-0778f90e7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexSyntheiaAgent:\n",
    "    \"\"\"Main AI Legal Assistant Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, deepseek_api_key: str, openai_api_key: str = None):\n",
    "        self.deepseek_client = DeepSeekClient(deepseek_api_key)\n",
    "        self.rag_system = RAGSystem()\n",
    "        \n",
    "        # Set OpenAI API key for embeddings\n",
    "        if openai_api_key:\n",
    "            os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "        \n",
    "        # Legal-specific system prompt\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are LexSyntheia, an advanced AI legal assistant designed to help lawyers and legal professionals with various legal tasks. Your capabilities include:\n",
    "\n",
    "        1. Legal Research: Analyze statutes, case law, and regulations\n",
    "        2. Contract Analysis: Review and analyze legal agreements\n",
    "        3. Compliance Checking: Assess regulatory compliance\n",
    "        4. Document Drafting: Assist with legal document preparation\n",
    "        5. Case Strategy: Provide insights for legal strategies\n",
    "\n",
    "        Guidelines:\n",
    "        - Always provide accurate, well-researched legal information\n",
    "        - Cite relevant sources and legal precedents\n",
    "        - Clearly distinguish between legal facts and opinions\n",
    "        - Highlight potential risks and considerations\n",
    "        - Provide practical recommendations\n",
    "        - Use clear, professional language\n",
    "        - Always include appropriate disclaimers about seeking professional legal advice\n",
    "\n",
    "        Remember: You are an AI assistant and do not replace professional legal counsel.\n",
    "        \"\"\"\n",
    "    \n",
    "    def initialize_knowledge_base(self, documents_path: str):\n",
    "        \"\"\"Initialize the RAG knowledge base\"\"\"\n",
    "        self.rag_system.build_knowledge_base(documents_path)\n",
    "    \n",
    "    def load_knowledge_base(self, path: str):\n",
    "        \"\"\"Load existing knowledge base\"\"\"\n",
    "        self.rag_system.load_knowledge_base(path)\n",
    "    \n",
    "    async def process_legal_query(self, legal_query: LegalQuery) -> LegalResponse:\n",
    "        \"\"\"Process a legal query and return a comprehensive response\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # Retrieve relevant documents\n",
    "            relevant_docs = self.rag_system.retrieve_relevant_documents(\n",
    "                legal_query.query_text, k=5\n",
    "            )\n",
    "            \n",
    "            # Prepare context from retrieved documents\n",
    "            context = self._prepare_context(relevant_docs)\n",
    "            \n",
    "            # Create enhanced prompt\n",
    "            enhanced_prompt = self._create_enhanced_prompt(\n",
    "                legal_query, context\n",
    "            )\n",
    "            \n",
    "            # Generate response using DeepSeek R1\n",
    "            response_text = await self.deepseek_client.generate_response(\n",
    "                enhanced_prompt, \n",
    "                self.system_prompt,\n",
    "                temperature=0.1,\n",
    "                max_tokens=2000\n",
    "            )\n",
    "            \n",
    "            # Extract legal citations and recommendations\n",
    "            legal_citations = self._extract_legal_citations(response_text)\n",
    "            recommendations = self._extract_recommendations(response_text)\n",
    "            \n",
    "            # Prepare sources information\n",
    "            sources = [\n",
    "                {\n",
    "                    \"source\": doc.metadata.get(\"source\", \"Unknown\"),\n",
    "                    \"document_type\": doc.metadata.get(\"document_type\", \"general\"),\n",
    "                    \"jurisdiction\": doc.metadata.get(\"jurisdiction\", \"unknown\")\n",
    "                }\n",
    "                for doc in relevant_docs\n",
    "            ]\n",
    "            \n",
    "            # Calculate confidence score\n",
    "            confidence_score = self._calculate_confidence_score(\n",
    "                relevant_docs, legal_query.query_text\n",
    "            )\n",
    "            \n",
    "            processing_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            return LegalResponse(\n",
    "                query_id=legal_query.query_id,\n",
    "                response_text=response_text,\n",
    "                confidence_score=confidence_score,\n",
    "                sources=sources,\n",
    "                legal_citations=legal_citations,\n",
    "                recommendations=recommendations,\n",
    "                timestamp=datetime.now(),\n",
    "                processing_time=processing_time\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing legal query: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _prepare_context(self, relevant_docs: List[Document]) -> str:\n",
    "        \"\"\"Prepare context from retrieved documents\"\"\"\n",
    "        if not relevant_docs:\n",
    "            return \"No relevant legal documents found in the knowledge base.\"\n",
    "        \n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(relevant_docs, 1):\n",
    "            source = doc.metadata.get(\"source\", \"Unknown Source\")\n",
    "            doc_type = doc.metadata.get(\"document_type\", \"general\")\n",
    "            \n",
    "            context_parts.append(\n",
    "                f\"[Document {i} - {doc_type.title()}]\\n\"\n",
    "                f\"Source: {source}\\n\"\n",
    "                f\"Content: {doc.page_content}\\n\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _create_enhanced_prompt(self, legal_query: LegalQuery, context: str) -> str:\n",
    "        \"\"\"Create an enhanced prompt with context and query details\"\"\"\n",
    "        return f\"\"\"\n",
    "        Legal Query Analysis Request:\n",
    "        \n",
    "        Query Type: {legal_query.query_type}\n",
    "        Jurisdiction: {legal_query.jurisdiction}\n",
    "        Urgency: {legal_query.urgency}\n",
    "        \n",
    "        Query: {legal_query.query_text}\n",
    "        \n",
    "        Relevant Legal Context:\n",
    "        {context}\n",
    "        \n",
    "        Please provide a comprehensive legal analysis that includes:\n",
    "        1. Direct answer to the query\n",
    "        2. Relevant legal principles and precedents\n",
    "        3. Potential risks and considerations\n",
    "        4. Practical recommendations\n",
    "        5. Next steps or actions to consider\n",
    "        \n",
    "        Format your response with clear sections and cite specific sources where applicable.\n",
    "        Include appropriate legal disclaimers.\n",
    "        \"\"\"\n",
    "    \n",
    "    def _extract_legal_citations(self, response_text: str) -> List[str]:\n",
    "        \"\"\"Extract legal citations from the response\"\"\"\n",
    "        # Simple regex patterns for common legal citations\n",
    "        import re\n",
    "        \n",
    "        citation_patterns = [\n",
    "            r'\\d+\\s+U\\.S\\.C\\.?\\s+§?\\s*\\d+',  # USC citations\n",
    "            r'\\d+\\s+F\\.\\d+d?\\s+\\d+',          # Federal court cases\n",
    "            r'\\d+\\s+S\\.Ct\\.\\s+\\d+',           # Supreme Court cases\n",
    "            r'[A-Z][a-z]+\\s+v\\.\\s+[A-Z][a-z]+',  # Case names\n",
    "        ]\n",
    "        \n",
    "        citations = []\n",
    "        for pattern in citation_patterns:\n",
    "            matches = re.findall(pattern, response_text)\n",
    "            citations.extend(matches)\n",
    "        \n",
    "        return list(set(citations))  # Remove duplicates\n",
    "    \n",
    "    def _extract_recommendations(self, response_text: str) -> List[str]:\n",
    "        \"\"\"Extract recommendations from the response\"\"\"\n",
    "        # Look for recommendation sections or bullet points\n",
    "        import re\n",
    "        \n",
    "        # Find lines that start with recommendation indicators\n",
    "        recommendation_patterns = [\n",
    "            r'(?i)recommend[a-z]*:?\\s*(.+)',\n",
    "            r'(?i)suggest[a-z]*:?\\s*(.+)',\n",
    "            r'(?i)advise[a-z]*:?\\s*(.+)',\n",
    "            r'^\\s*[-•]\\s*(.+)$'  # Bullet points\n",
    "        ]\n",
    "        \n",
    "        recommendations = []\n",
    "        lines = response_text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            for pattern in recommendation_patterns:\n",
    "                matches = re.findall(pattern, line)\n",
    "                if matches:\n",
    "                    recommendations.extend(matches)\n",
    "        \n",
    "        return recommendations[:5]  # Limit to top 5 recommendations\n",
    "    \n",
    "    def _calculate_confidence_score(self, relevant_docs: List[Document], query: str) -> float:\n",
    "        \"\"\"Calculate confidence score based on retrieved documents relevance\"\"\"\n",
    "        if not relevant_docs:\n",
    "            return 0.0\n",
    "        \n",
    "        # Simple heuristic based on number and quality of retrieved documents\n",
    "        base_score = min(len(relevant_docs) * 0.15, 0.75)  # Up to 0.75 for having docs\n",
    "        \n",
    "        # Boost for specific document types\n",
    "        for doc in relevant_docs:\n",
    "            doc_type = doc.metadata.get(\"document_type\", \"general\")\n",
    "            if doc_type in [\"statute\", \"case_law\"]:\n",
    "                base_score += 0.05\n",
    "            elif doc_type == \"regulation\":\n",
    "                base_score += 0.03\n",
    "        \n",
    "        return min(base_score, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d961b-8acf-4ef8-a2e1-44dbddfa0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Example and Testing\n",
    "async def main():\n",
    "    \"\"\"Example usage of the LexSyntheia Legal Assistant\"\"\"\n",
    "    \n",
    "    # Initialize the agent\n",
    "    agent = LexSyntheiaAgent(\n",
    "        deepseek_api_key=\"sk-3d2f92176d02496cb78c8965f1739d2d\",\n",
    "        openai_api_key=\"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\"\n",
    "    )\n",
    "    \n",
    "    # Initialize or load knowledge base\n",
    "    # agent.initialize_knowledge_base(\"./legal_documents\")\n",
    "    # agent.rag_system.save_knowledge_base(\"./knowledge_base\")\n",
    "    \n",
    "    # Create a sample legal query\n",
    "    query = LegalQuery(\n",
    "        query_id=\"query_001\",\n",
    "        user_id=\"lawyer_123\",\n",
    "        query_text=\"What are the key elements required for a valid contract under US law?\",\n",
    "        query_type=\"legal_research\",\n",
    "        jurisdiction=\"federal\",\n",
    "        urgency=\"medium\",\n",
    "        timestamp=datetime.now()\n",
    "    )\n",
    "    \n",
    "    # Process the query\n",
    "    try:\n",
    "        response = await agent.process_legal_query(query)\n",
    "        \n",
    "        print(\"=== LexSyntheia Legal Assistant Response ===\")\n",
    "        print(f\"Query ID: {response.query_id}\")\n",
    "        print(f\"Confidence Score: {response.confidence_score:.2f}\")\n",
    "        print(f\"Processing Time: {response.processing_time:.2f}s\")\n",
    "        print(f\"\\nResponse:\\n{response.response_text}\")\n",
    "        print(f\"\\nSources: {len(response.sources)} documents referenced\")\n",
    "        print(f\"Legal Citations: {response.legal_citations}\")\n",
    "        print(f\"Recommendations: {response.recommendations}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
